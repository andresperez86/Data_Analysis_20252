{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e42fc2ef",
   "metadata": {},
   "source": [
    "# 4_2_Example_Data_Bank — **pandas API on Spark** version\n",
    "\n",
    "This notebook demonstrates how to run your pandas-style workflow on top of Spark using the **pandas API on Spark** (`pyspark.pandas`), taking into account the Quickstart you referenced.\n",
    "\n",
    "It includes:\n",
    "- A quick primer (object creation, conversions between pandas ↔ pandas-on-Spark ↔ Spark DataFrame).\n",
    "- Ready-to-use I/O patterns (`ps.read_csv`, `psdf.to_parquet`, Spark I/O).\n",
    "- Practical tips to **port your existing pandas notebook** with minimal code changes (mostly `import pandas as pd` → `import pyspark.pandas as ps`).\n",
    "\n",
    "> If you already have a Spark cluster/session (Databricks, local Spark, EMR, etc.), run the cells below as-is.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6c8d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyspark.pandas as ps\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Start or get Spark\n",
    "spark = SparkSession.builder.appName(\"BankExample-pandas-on-Spark\").getOrCreate()\n",
    "\n",
    "# Optional: tune options (Arrow + distributed index)\n",
    "prev_arrow = spark.conf.get(\"spark.sql.execution.arrow.pyspark.enabled\", \"false\")\n",
    "ps.set_option(\"compute.default_index_type\", \"distributed\")  # lighter index by default\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", True)\n",
    "\n",
    "print(\"Spark version:\", spark.version)\n",
    "print(\"Arrow enabled:\", spark.conf.get(\"spark.sql.execution.arrow.pyspark.enabled\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e326b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Object Creation (from your Quickstart) ---\n",
    "\n",
    "# pandas-on-Spark Series\n",
    "s = ps.Series([1, 3, 5, np.nan, 6, 8])\n",
    "display(s)\n",
    "\n",
    "# pandas-on-Spark DataFrame\n",
    "psdf = ps.DataFrame(\n",
    "    {'a': [1, 2, 3, 4, 5, 6],\n",
    "     'b': [100, 200, 300, 400, 500, 600],\n",
    "     'c': [\"one\", \"two\", \"three\", \"four\", \"five\", \"six\"]},\n",
    "    index=[10, 20, 30, 40, 50, 60]\n",
    ")\n",
    "display(psdf)\n",
    "\n",
    "# pandas DataFrame → pandas-on-Spark\n",
    "dates = pd.date_range('20130101', periods=6)\n",
    "pdf = pd.DataFrame(np.random.randn(6, 4), index=dates, columns=list('ABCD'))\n",
    "psdf2 = ps.from_pandas(pdf)\n",
    "display(psdf2.head())\n",
    "\n",
    "# pandas DataFrame → Spark DataFrame\n",
    "sdf = spark.createDataFrame(pdf)\n",
    "sdf.show()\n",
    "\n",
    "# Spark DataFrame → pandas-on-Spark DataFrame\n",
    "psdf3 = sdf.pandas_api()\n",
    "display(psdf3.head())\n",
    "\n",
    "# dtypes, index, columns, numpy view\n",
    "display(psdf3.dtypes)\n",
    "display(psdf3.index)\n",
    "display(psdf3.columns)\n",
    "display(psdf3.to_numpy())\n",
    "\n",
    "# describe, transpose, sort\n",
    "display(psdf3.describe())\n",
    "display(psdf3.T)\n",
    "display(psdf3.sort_index(ascending=False).head())\n",
    "display(psdf3.sort_values(by='B').head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30802beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Missing Data ---\n",
    "pdf1 = pdf.reindex(index=dates[0:4], columns=list(pdf.columns) + ['E'])\n",
    "pdf1.loc[dates[0]:dates[1], 'E'] = 1\n",
    "psdf1 = ps.from_pandas(pdf1)\n",
    "display(psdf1)\n",
    "\n",
    "display(psdf1.dropna(how='any'))\n",
    "display(psdf1.fillna(value=5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24020a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Grouping ---\n",
    "psdf_g = ps.DataFrame({'A': ['foo', 'bar', 'foo', 'bar', 'foo', 'bar', 'foo', 'foo'],\n",
    "                       'B': ['one', 'one', 'two', 'three', 'two', 'two', 'one', 'three'],\n",
    "                       'C': np.random.randn(8),\n",
    "                       'D': np.random.randn(8)})\n",
    "\n",
    "display(psdf_g)\n",
    "display(psdf_g.groupby('A').sum())\n",
    "display(psdf_g.groupby(['A', 'B']).sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aea3650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Plotting ---\n",
    "# For large data, consider sampling before plotting.\n",
    "pser = pd.Series(np.random.randn(1000), index=pd.date_range('1/1/2000', periods=1000))\n",
    "psser = ps.Series(pser).cummax()\n",
    "ax = psser.plot()\n",
    "ax.set_title(\"Series cummax()\")\n",
    "\n",
    "pdf_plot = pd.DataFrame(np.random.randn(1000, 4), index=pser.index, columns=['A', 'B', 'C', 'D'])\n",
    "psdf_plot = ps.from_pandas(pdf_plot).cummax()\n",
    "ax2 = psdf_plot.plot()\n",
    "ax2.set_title(\"DataFrame cummax()\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094b8efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Getting data in/out ---\n",
    "\n",
    "# CSV (read/write)\n",
    "# Replace 'path/to/file.csv' with your file path (local, DBFS, s3://, abfss://, etc.)\n",
    "# Read\n",
    "# ps.read_csv supports common pandas-like options.\n",
    "# bank_psdf = ps.read_csv('path/to/file.csv')\n",
    "# Write\n",
    "# bank_psdf.to_csv('foo.csv')\n",
    "\n",
    "# Parquet (recommended for speed + schema)\n",
    "# bank_psdf.to_parquet('bar.parquet')\n",
    "# ps.read_parquet('bar.parquet')\n",
    "\n",
    "# Spark I/O interop (ORC, JDBC, etc.)\n",
    "# bank_psdf.spark.to_spark_io('zoo.orc', format=\"orc\")\n",
    "# ps.read_spark_io('zoo.orc', format=\"orc\")\n",
    "\n",
    "# Spark DataFrame interop at any time:\n",
    "# sdf_bank = bank_psdf.to_spark()          # pandas-on-Spark -> Spark DF\n",
    "# bank_psdf2 = sdf_bank.pandas_api()       # Spark DF -> pandas-on-Spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640bcad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Porting your existing pandas notebook ---\n",
    "# 1) Replace imports (where possible):\n",
    "#    import pandas as pd    -> keep if you still use pandas locally\n",
    "#    import pyspark.pandas as ps    # new: use ps everywhere you used pandas ops on big data\n",
    "#\n",
    "# 2) Use ps.read_csv / ps.read_parquet instead of pd.read_csv/pd.read_parquet for big data.\n",
    "#\n",
    "# 3) Many pandas ops work the same: .head(), .describe(), .groupby(), .fillna(), .dropna(), .merge(), .sort_values(), etc.\n",
    "#\n",
    "# 4) For plotting or ops requiring in-memory arrays, use small samples, or convert via .to_pandas() on small/filtered data:\n",
    "#    psdf.sample(frac=0.05, random_state=42).to_pandas().plot(...)\n",
    "#\n",
    "# 5) If you already have a Spark DataFrame 'df' from previous steps, instantly bridge to pandas-on-Spark:\n",
    "#    psdf = df.pandas_api()\n",
    "#\n",
    "# 6) Ordered head (preserve natural order) adds sorting overhead. Enable only if needed:\n",
    "#    ps.set_option('compute.ordered_head', True)   # default False\n",
    "#\n",
    "# 7) Arrow acceleration is already enabled above. You can toggle if needed:\n",
    "#    spark.conf.set('spark.sql.execution.arrow.pyspark.enabled', True)\n",
    "#\n",
    "# 8) When something isn't supported in pandas-on-Spark, drop down to Spark DataFrame APIs:\n",
    "#    sdf = psdf.to_spark()\n",
    "#    # do Spark transformations...\n",
    "#    psdf = sdf.pandas_api()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d7c569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- (Optional) Reset options at the end ---\n",
    "# ps.reset_option(\"compute.default_index_type\")\n",
    "# spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", prev_arrow)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
